{
  "hash": "d0aea65824932e3fbe0af36f3baf7a97",
  "result": {
    "markdown": "---\ntitle: Learn RL Fundamentals and Vocab in Five Minutes\nauthor: Jay Lowe\ndate: '2022-10-08'\ncategories:\n  - RL\n  - Beginner\nimage: image.jpg\nformat:\n  html:\n    code-fold: false\n---\n\n - TODO: when to be greedy, exploitation vs exploration explained\n\n - TODO: action value methods?\n\n##### Article Summary\n\n> Learn all the reinforcement learning (RF) fundamental concepts and terminalogy such as `reward` and `value function` in five minutes.\n\nTo help us better understand RL, I will be pairing technical writing with a simple analogy comparing an RL algorithm to that of a three year old.\n\nMy daughter was three at the time of this writing and it was fun explaining RL concepts to my non-technical wife as the kid jumps around and yells, so hopefully it helps you as well.\n\n# Core concepts\n\nGiven an `environment` and a set of `actions` that can be performed in that environment, an RL algorithm learns how to maximize `reward` within the context of a measurable `goal`. \n\nHearby, the RL algorithm performing actions shall be known as the `agent`.\n\n## Meet the agent\n\nTODO - meet agent Rae?\n\nMy three year old `agent` Rae has a bedroom containing various features including toys, books, a bed, and much more.\n\nInside her bedroom `environment`, she can perform many `actions` such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\n\nDepending on if she wants to play or go to sleep--the `goal` set for her in this bedroom will determine which actions lead high `rewards`. \n\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high `reward` while getting jacked up on blocks falling would produce low `reward` (as they rile her up).\n\n## What can you solve with reinforcement learning?\n\nProblem statements with a well defined environment and bounded set of actions make can typically be solved using RL methods.\n\nSome examples of bounded problem statements include:\n- Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors)\n- Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game\n- Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it \n- Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\n\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured.\n\n# Additional components of an RL algorithm\n\nNow that we know the basics, we must capture some additional nuances required to effectively communicate to an algorithm what it must accomplish.\n\n## How an RL agent knows which actions to take\n\nAn `environment` can be used in many different ways depending on the `goal` an `agent` has at the time (imagine all the ways you can use your kitchent).\n\nThe `policy` determines which `actions` lead to the best outcome by mapping all `actions` possible, given the state of the `environment`, to a known `reward` value.\n\nFor example, an agent such as Rae operating in her bedroom will have seperate `policies` when playing vs going to sleep. Her playtime `policy` will put rewards on actions related to her toys while her sleep `policy` will place a reward on actions that calm her down.\n\n## Actions have results\n\nAfter taking an `action` in its `environment`, the `agent` will measure how much progress it made towards its `goal`.\n\nThe amount of progess made takes the form of a singular number known as the `reward signal`. RL agents exist to find the path to maximum `reward`.\n\nRL agents will sometimes use the resulting `reward` to alter the `policy` mappings.\n\nIf Rae finds that playing with a toy in a particular way was especially fun when using her playtime `policy`, she may value that `action` more next she plays.\n\n## Rewards have both an immedate and long term payoff\n\nHow do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow--which one leads to the most long term `reward`?\n\nThe `value function` bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.\n\nRL agents will always attempt to find the optimal `policy` that leads to optimal `rewards`.\n\nComing back to Rae's playtime `policy`, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place--upon which a massive `reward` spike hits and she has the most fun possible.\n\nDimming the lights and reading stories during her sleeping `policy` might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep `action` occuring.\n\n**An effective `value function` represents effective estimation of value--the single most important component to an RL agent--as this leads to accurate mappings of `actions` to `reward`.**\n\n## Planning for future actions\n\nSometimes an RL agent will have access to an `environment` `model` that estimates the results of an `action`.\n\nA `model` may not always be available but can be particularly useful for games, or simple physics `environments`, in which clear causation exists.\n\nWhen I'm teaching Rae about physical phenomona such as the water cycle, I will often employ a `model` that includes a temperature scale and the states of water for experimentation.\n\n## The difference between reinforcment learning, supervised learning, and unsupervised learning\n\nSupervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.\n\nUnsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.\n\nUnlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.\n\n### RL can be combined with other ML techniques\n\nRL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.\n\nI think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.\n\n\n#### Deep reinforcment learning explained\n\nUse a deep neural network for your `policy` and you have deep RL.\n\nTODO - Finish\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}