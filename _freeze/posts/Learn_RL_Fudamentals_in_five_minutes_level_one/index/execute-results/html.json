{
  "hash": "fe3628b91df112198da7c23b8e02e12b",
  "result": {
    "markdown": "---\ntitle: Learn RL Fundamentals in Five Minutes (Level 1)\nauthor: Jay Lowe\ndate: '2023-01-07'\ncategories:\n  - RL\n  - Beginner\nimage: rl_agent_level_1.jpeg\nanchor-sections: true\nformat:\n  html:\n    code-fold: false\n    smooth-scroll: true\n---\n\n##### Article Summary\n\n> Learn all the reinforcement learning (RL) fundamental concepts and terminology such as `reward` and `value function` in five minutes.\n\nTo help us better understand RL, I will be pairing technical writing with a simple analogy comparing an RL algorithm to that of a three year old.\n\nMy daughter was three at the time of this writing and it was fun explaining RL concepts to my non-technical wife as the kid jumps around and yells, so hopefully it helps you as well.\n\n# Core concepts\n\nGiven an `environment` and a set of `actions` that can be performed in that environment, an RL algorithm learns how to maximize `reward` within the context of a measurable `goal`. \n\nHearby, the RL algorithm performing actions shall be known as the `agent`.\n\n## Meet the agent\n\nMy three year old `agent` Rae has a bedroom containing various features including toys, books, a bed, and much more.\n\nInside her bedroom `environment`, she can perform many `actions` such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.\n\nDepending on if she wants to play or go to sleep--the `goal` set for her in this bedroom will determine which actions lead high `rewards`. \n\nFor example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high `reward` while violently jumping on the bed would produce low `reward` (as `goal` == going to bed). If she wanted to play, which would probably be the case without intervention, then jumping on the bed would produce a high `reward`. \n\n## What can you solve with reinforcement learning?\n\nProblem statements with a well defined environment and bounded set of actions can typically be solved using RL methods.\n\nSome examples of bounded problem statements include:\n- Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors)\n- Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game\n- Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it \n- Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it\n\nEssentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured.\n\n## Conclusion\n\nTest yourself with the following questions:\n- \"Can everything truly be solved with by RL? If yes, list out some situations that would be impossible for an RL agent to work in.\"\n- \"Define an `environment`, set of `actions`, an achevialbe `goal` that can be reached through those `actions`, and a relevant `reward` system to measure progress on that goal.\"\n\nTweet your answer to me at @ogjaylowe so we can have a chat about it! Would love to discuss.\n\n## What to read next\n\nGot a hang for the fundamentals of RL and looking for more to read? Check out my post on [additional complexities and components of an RL algorithm](../Learn_RL_Fudamentals_in_five_minutes_level_two/index.qmd) , or if you want to start coding, read about [creating a simple bandit](../Creating_Simple_N-Arm_Bandits_for_RL_Agents/index.qmd).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}