{
  "hash": "2ff69a72f9e1e9da4c85658a34aeeea5",
  "result": {
    "markdown": "---\ntitle: Creating Simple N-Arm Bandits for RL Agents\nauthor: Jay Lowe\ndate: '2022-19-08'\ncategories:\n  - RL\n  - Code\n  - Bandit\nimage: image.jpg\nformat:\n  html:\n    code-fold: false\n---\n\n##### Article Summary\n\n> Learn how to create a simple N-armed bandit and train an RL agent to its optimal policy and value function.\n\n# Creating an n-armed bandit\n\n## Defining a bandit\n\nA bandit represents an environment, a set of rewards, and a set of actions. \n\nFor example, a one armed bandit has one possible action (one \"arm\" or lever) in its environment and pulling that arm generates one set of rewards--typically as a randomly generated number between two set intervals such as 0 and 1.\n\n## Why bandits?\n\nRL agents use evaluation methods to dictate what actions it takes, instead of instruction from a combination of loss/reward/etc. functions.\n\nGuiding an RL agent through evaluative feedback will help it understand which actions provide the most reward but doesn't specify which action provides the best or worst outcomes.\n\nBandits allow us to create simple test beds for training RL agents in. An RL agent must learn to maximize total reward when interacting with the bandit given a number of action selections.\n\nIf your bandit has three arms and the RL agent can choose to pull one of those three levers 1000 times--which combination of lever pulls will lead to the highest possible reward? An effective RL agent should learn the optimal sequence of when and which levers to pull.\n\n## Providing bandit actions a value\n\nIn life and in RL, if we had a perfect understanding of the short and long term value tied to an action we would be able to `exploit` that to our advantage.\n\nLet's create some perfect ground truth values for a three armed bandit.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# assign a random starting seed value\nnp.random.seed(5)\n\n# basis for generating the reward ground truths\nmean = 0          # also known as mu\nstandard_deviation = 1      # also known as sigma\narms = 3\n\n# bandit values\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 1000\n\n# plot initial ground truth values\nplt.plot(action_range, reward_truths, 'o', color='black')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=589 height=449}\n:::\n:::\n\n\nUnfortunately, we don't have perfect knowledge so we as agents must do our best to estimate the reward value of an action before we take it.\n\nWe can't provide a static ground truth value for a bandit arm or else a `greedy` RL agent will always be able to quickly solve the problem in a way that doesn't replicate real world situations.\n\n### A better action-value method\n\nA good bandit arm should be assigned a set reward value to act as the ground truth, a range of possible reward values to pull from anchored on the ground truth, and the resulting reward should be randomly sampled from that range when the arm gets pulled.\n\nI like to think of this as applying a standard deviation error bar to your starting point.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# apply a standard deviation error bar to the ground truth values\nplt.errorbar(action_range, reward_truths, np.ones(arms), fmt='o')\n\n# plot details\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Deviation Range')\nplt.show()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=587 height=449}\n:::\n:::\n\n\nIn implementation, the agent will use a properly sampled distribution of actions and not a deviation bar.  \n\nLet's update the the visualization of each bandit arm with 1000 sampled data points to better capture these good practices.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=587 height=449}\n:::\n:::\n\n\nFor each additional bandit arm we add, the same process will occur. Check out a 15 arm bandit, with twice the standard deviation, that as 2000 total action \"time steps\".\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# updated bandit values\narms = 15\nstandard_deviation = 2\naction_range = np.arange(0, arms)\nreward_truths = np.random.normal(mean, standard_deviation, (arms))\ntotal_actions_allowed = 2000\n\n# for each arm's reward truth, generate distribution between 1 and total_actions_allowed\nreward_ranges = np.array([np.random.normal(true_reward,1,total_actions_allowed) for true_reward in reward_truths])\n\n# plot scatter points representing the sampled value range centered on ground truth value\nfor i in action_range:\n    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])\n\n# plot ground truth ranges\nplt.plot(action_range, reward_truths,'o', color='black')\nplt.xlabel('Arm Number')\nplt.ylabel('Arm Value')\nplt.xticks(range(0,arms))\nplt.title('Initial Bandit Arm Ground Truth Values with Sampling Applied')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=587 height=449}\n:::\n:::\n\n\nThe wider range of values to sample from and increased number of arms increase the complexity, thereby making it harder for the agent to find the optimal value function.\n\n# Next up\n\nNow that we know how to create a simple n-armed bandit, we need to build an RL agent capable of maximizing reward during interactions.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}