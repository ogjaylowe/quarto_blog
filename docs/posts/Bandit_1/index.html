<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jay Lowe">
<meta name="dcterms.date" content="2022-10-08">

<title>Jay Lowe‚Äôs Blog - Creating and Solving Simple N-Arm Bandits with RL Agents</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jay Lowe‚Äôs Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Creating and Solving Simple N-Arm Bandits with RL Agents</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">RL</div>
                <div class="quarto-category">Code</div>
                <div class="quarto-category">Bandit</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jay Lowe </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 8, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<ul>
<li>evaluation methods explained</li>
</ul>
<section id="article-summary" class="level5">
<h5 class="anchored" data-anchor-id="article-summary">Article Summary</h5>
<blockquote class="blockquote">
<p>Learn how to create a simple N-armed bandit and train an RL agent to its optimal policy and value function.</p>
</blockquote>
</section>
<section id="creating-an-n-armed-bandit" class="level1">
<h1>Creating an n-armed bandit</h1>
<section id="defining-a-bandit" class="level2">
<h2 class="anchored" data-anchor-id="defining-a-bandit">Defining a bandit</h2>
<p>A bandit represents an environment, a set of rewards, and a set of actions.</p>
<p>For example, a one armed bandit has one possible action (one ‚Äúarm‚Äù or lever) in its environment and pulling that arm generates one set of rewards‚Äìtypically as a randomly generated number between two set intervals such as 0 and 1.</p>
</section>
<section id="why-bandits" class="level2">
<h2 class="anchored" data-anchor-id="why-bandits">Why bandits?</h2>
<p>RL agents use evaluation methods to dictate what actions it takes, instead of instruction from a combination of loss/reward/etc. functions.</p>
<p>Guiding an RL agent through evaluative feedback will help it understand which actions provide the most reward but doesn‚Äôt specify which action provides the best or worst outcomes.</p>
<p>Bandits allow us to create simple test beds for training RL agents in. An RL agent must learn to maximize total reward when interacting with the bandit given a number of action selections.</p>
<p>If your bandit has three arms and the RL agent can choose to pull one of those three levers 1000 times‚Äìwhich combination of lever pulls will lead to the highest possible reward? An effective RL agent should learn the optimal sequence of when and which levers to pull.</p>
</section>
<section id="providing-bandit-actions-a-value" class="level2">
<h2 class="anchored" data-anchor-id="providing-bandit-actions-a-value">Providing bandit actions a value</h2>
<p>In life and in RL, if we had a perfect understanding of the short and long term value tied to an action we would be able to <code>exploit</code> that to our advantage.</p>
<p>Let‚Äôs create some perfect ground truth values for a three armed bandit.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># assign a random starting seed value</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">5</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># basis for generating the reward ground truths</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> <span class="dv">0</span>          <span class="co"># also known as mu</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>standard_deviation <span class="op">=</span> <span class="dv">1</span>      <span class="co"># also known as sigma</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>arms <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># bandit values</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>action_range <span class="op">=</span> np.arange(<span class="dv">0</span>, arms)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>reward_truths <span class="op">=</span> np.random.normal(mean, standard_deviation, (arms))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>total_actions_allowed <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># plot initial ground truth values</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.plot(action_range, reward_truths, <span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># plot details</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Arm Number'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Arm Value'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">0</span>,arms))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Bandit Arm Ground Truth Values'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="589" height="449"></p>
</div>
</div>
<p>Unfortunately, we don‚Äôt have perfect knowledge so we as agents must do our best to estimate the reward value of an action before we take it.</p>
<p>We can‚Äôt provide a static ground truth value for a bandit arm or else a <code>greedy</code> RL agent will always be able to quickly solve the problem in a way that doesn‚Äôt replicate real world situations.</p>
<section id="a-better-action-value-method" class="level3">
<h3 class="anchored" data-anchor-id="a-better-action-value-method">A better action-value method</h3>
<p>A good bandit arm should be assigned a set reward value to act as the ground truth, a range of possible reward values to pull from anchored on the ground truth, and the resulting reward should be randomly sampled from that range when the arm gets pulled.</p>
<p>I like to think of this as applying a standard deviation error bar to your starting point.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># apply a standard deviation error bar to the ground truth values</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.errorbar(action_range, reward_truths, np.ones(arms), fmt<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot details</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Arm Number'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Arm Value'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">0</span>,arms))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Bandit Arm Ground Truth Values with Deviation Range'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="587" height="449"></p>
</div>
</div>
<p>In implementation, the agent will use a properly sampled distribution of actions and not a deviation bar.</p>
<p>Let‚Äôs update the the visualization of each bandit arm with 1000 sampled data points to better capture these good practices.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for each arm's reward truth, generate distribution between 1 and total_actions_allowed</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>reward_ranges <span class="op">=</span> np.array([np.random.normal(true_reward,<span class="dv">1</span>,total_actions_allowed) <span class="cf">for</span> true_reward <span class="kw">in</span> reward_truths])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot scatter points representing the sampled value range centered on ground truth value</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> action_range:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot ground truth ranges</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.plot(action_range, reward_truths,<span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Arm Number'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Arm Value'</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">0</span>,arms))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Bandit Arm Ground Truth Values with Sampling Applied'</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="587" height="449"></p>
</div>
</div>
<p>For each additional bandit arm we add, the same process will occur. Check out a 15 arm bandit, with twice the standard deviation, that as 2000 total action ‚Äútime steps‚Äù.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># updated bandit values</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>arms <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>standard_deviation <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>action_range <span class="op">=</span> np.arange(<span class="dv">0</span>, arms)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>reward_truths <span class="op">=</span> np.random.normal(mean, standard_deviation, (arms))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>total_actions_allowed <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># for each arm's reward truth, generate distribution between 1 and total_actions_allowed</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>reward_ranges <span class="op">=</span> np.array([np.random.normal(true_reward,<span class="dv">1</span>,total_actions_allowed) <span class="cf">for</span> true_reward <span class="kw">in</span> reward_truths])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># plot scatter points representing the sampled value range centered on ground truth value</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> action_range:</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    plt.scatter(np.full((total_actions_allowed),i),reward_ranges[i])</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># plot ground truth ranges</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.plot(action_range, reward_truths,<span class="st">'o'</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Arm Number'</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Arm Value'</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">0</span>,arms))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Initial Bandit Arm Ground Truth Values with Sampling Applied'</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="587" height="449"></p>
</div>
</div>
<p>The wider range of values to sample from and increased number of arms increase the complexity, thereby making it harder for the agent to find the optimal value function.</p>
</section>
</section>
</section>
<section id="next-up" class="level1">
<h1>Next up</h1>
<p>Now that we know how to create a simple n-armed bandit, we need to build an RL agent capable of maximizing reward during interactions.</p>
</section>
<section id="solving-a-bandit" class="level1">
<h1>Solving a bandit?</h1>
<section id="greedy-and-nongreedy-actions" class="level2">
<h2 class="anchored" data-anchor-id="greedy-and-nongreedy-actions">Greedy and nongreedy actions</h2>
<p>An agent will take a <code>greedy</code> action if it has been told to <code>exploit</code> its environment and choose the highest reward possible.</p>
<p>It will choose the <code>nongreedy</code> action during <code>exploration</code> in an attempt to better estimate the reward value an action will provide.</p>
<p>One should <code>exploit</code> once the optimal policy and value functions have been determined, as that will lead to maximum rewards.</p>
<p>Identifying the right ratio of <code>exploitaton</code> to <code>exploration</code> and the <code>exploration</code> decay rate plays a critical role in a succesfully converging an RL agent to optimal performance.</p>
</section>
<section id="meet-Œµ" class="level2">
<h2 class="anchored" data-anchor-id="meet-Œµ">Meet Œµ</h2>
<p>The ratio of when the agent chooses to <code>exploit</code> or <code>explore</code> commonly gets denoted with the greek notation of epsilon Œµ.</p>
<p>In general, <code>exploration</code> should be occur drastically less then <code>exploit</code>, as the agent should be focused on finding the maximum reward, so usually we set Œµ to equal to .1 or less.</p>
<section id="why-exploration-works" class="level3">
<h3 class="anchored" data-anchor-id="why-exploration-works">Why <code>exploration</code> works</h3>
<p>If you were to give an agent infinite resources and an Œµ value greater then zero, as it approaches the limit of infinity, it will exhaust all possible actions and thereby discover the ground truth values of a bandit.</p>
<p>Agents do not have infinite resources but give them enough time and they will approach the limit close enough to accurately estimate the ground truth value to a certain acceptable degree.</p>
<hr>
</section>
</section>
</section>
<section id="core-concepts" class="level1">
<h1>Core concepts</h1>
<p>Given an <code>environment</code> and a set of <code>actions</code> that can be performed in that environment, an RL algorithm learns how to maximize <code>reward</code> within the context of a measurable <code>goal</code>.</p>
<p>Hearby, the RL algorithm performing actions shall be known as the <code>agent</code>.</p>
<section id="meet-the-agent" class="level2">
<h2 class="anchored" data-anchor-id="meet-the-agent">Meet the agent</h2>
<p>TODO - meet agent Rae?</p>
<p>My three year old <code>agent</code> Rae has a bedroom containing various features including toys, books, a bed, and much more.</p>
<p>Inside her bedroom <code>environment</code>, she can perform many <code>actions</code> such as playing with her toys, reading the books, jumping or sleeping in the bed, etc.</p>
<p>Depending on if she wants to play or go to sleep‚Äìthe <code>goal</code> set for her in this bedroom will determine which actions lead high <code>rewards</code>.</p>
<p>For example, if whe wants to go to sleep then climbing into bed and getting tucked into the sheets would produce high <code>reward</code> while getting jacked up on blocks falling would produce low <code>reward</code> (as they rile her up).</p>
</section>
<section id="what-can-you-solve-with-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="what-can-you-solve-with-reinforcement-learning">What can you solve with reinforcement learning?</h2>
<p>Problem statements with a well defined environment and bounded set of actions make can typically be solved using RL methods.</p>
<p>Some examples of bounded problem statements include: - Robotics: robot appendanges typically have a limited range of motion and must move or interact with physical objects in a finite environment (typically dictated by their sensors) - Games: a game board has a defined state at any given point with a limited set of actions determined by the rules of the game - Cooking: given a well defined set of taste preferences as a goal, an RL agent can combine available ingredients with methods of cooking available to it - Stock market predictions: a market has a defined state at a given point of time and a limited number of ways to interact with it</p>
<p>Essentially, anything that has a limited set of actions in a defined environment could be jigged into a RL problem if progress towards a goal in that context can be measured.</p>
</section>
</section>
<section id="additional-components-of-an-rl-algorithm" class="level1">
<h1>Additional components of an RL algorithm</h1>
<p>Now that we know the basics, we must capture some additional nuances required to effectively communicate to an algorithm what it must accomplish.</p>
<section id="how-an-rl-agent-knows-which-actions-to-take" class="level2">
<h2 class="anchored" data-anchor-id="how-an-rl-agent-knows-which-actions-to-take">How an RL agent knows which actions to take</h2>
<p>An <code>environment</code> can be used in many different ways depending on the <code>goal</code> an <code>agent</code> has at the time (imagine all the ways you can use your kitchent).</p>
<p>The <code>policy</code> determines which <code>actions</code> lead to the best outcome by mapping all <code>actions</code> possible, given the state of the <code>environment</code>, to a known <code>reward</code> value.</p>
<p>For example, an agent such as Rae operating in her bedroom will have seperate <code>policies</code> when playing vs going to sleep. Her playtime <code>policy</code> will put rewards on actions related to her toys while her sleep <code>policy</code> will place a reward on actions that calm her down.</p>
</section>
<section id="actions-have-results" class="level2">
<h2 class="anchored" data-anchor-id="actions-have-results">Actions have results</h2>
<p>After taking an <code>action</code> in its <code>environment</code>, the <code>agent</code> will measure how much progress it made towards its <code>goal</code>.</p>
<p>The amount of progess made takes the form of a singular number known as the <code>reward signal</code>. RL agents exist to find the path to maximum <code>reward</code>.</p>
<p>RL agents will sometimes use the resulting <code>reward</code> to alter the <code>policy</code> mappings.</p>
<p>If Rae finds that playing with a toy in a particular way was especially fun when using her playtime <code>policy</code>, she may value that <code>action</code> more next she plays.</p>
</section>
<section id="rewards-have-both-an-immedate-and-long-term-payoff" class="level2">
<h2 class="anchored" data-anchor-id="rewards-have-both-an-immedate-and-long-term-payoff">Rewards have both an immedate and long term payoff</h2>
<p>How do RL agents handle differed rewards? If for example I offered you $1,000 today or $10,000 dollars tomorrow‚Äìwhich one leads to the most long term <code>reward</code>?</p>
<p>The <code>value function</code> bakes in discounted future rewards in conjunction with the immedate rewards to better represent which actions lead to the best long term outcome.</p>
<p>RL agents will always attempt to find the optimal <code>policy</code> that leads to optimal <code>rewards</code>.</p>
<p>Coming back to Rae‚Äôs playtime <code>policy</code>, taking the time to slowly build up a large tower of blocks might not be that fun until the last piece gets put into place‚Äìupon which a massive <code>reward</code> spike hits and she has the most fun possible.</p>
<p>Dimming the lights and reading stories during her sleeping <code>policy</code> might not immedately lead to sleep, as opposed to forcing her into bed, but they put her into a tired state that greatly increases the chances of the sleep <code>action</code> occuring.</p>
<p><strong>An effective <code>value function</code> represents effective estimation of value‚Äìthe single most important component to an RL agent‚Äìas this leads to accurate mappings of <code>actions</code> to <code>reward</code>.</strong></p>
</section>
<section id="planning-for-future-actions" class="level2">
<h2 class="anchored" data-anchor-id="planning-for-future-actions">Planning for future actions</h2>
<p>Sometimes an RL agent will have access to an <code>environment</code> <code>model</code> that estimates the results of an <code>action</code>.</p>
<p>A <code>model</code> may not always be available but can be particularly useful for games, or simple physics <code>environments</code>, in which clear causation exists.</p>
<p>When I‚Äôm teaching Rae about physical phenomona such as the water cycle, I will often employ a <code>model</code> that includes a temperature scale and the states of water for experimentation.</p>
</section>
<section id="the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="the-difference-between-reinforcment-learning-supervised-learning-and-unsupervised-learning">The difference between reinforcment learning, supervised learning, and unsupervised learning</h2>
<p>Supervised learning typically means we supply the algorithm with some form of training dataset that we have vetted as correct. For example, a computer vision model will be trained on pictures with human generated lables specifying what each object in the picture represents.</p>
<p>Unsupervised learning removes the human verified data and substitues it with a method to find hidden correlations and trends to create machine generated training data.</p>
<p>Unlike these traditional machine learning (ML) algorithms, RL algorithms do not utilize a training datset. Instead they attempt to maximize reward through repeated exploration and exploitation.</p>
<section id="rl-can-be-combined-with-other-ml-techniques" class="level3">
<h3 class="anchored" data-anchor-id="rl-can-be-combined-with-other-ml-techniques">RL can be combined with other ML techniques</h3>
<p>RL algorithms can utilize techniques found in un/supervised learning, and can also benefit from the introduction of sub-problems, but fundamentally does not require them.</p>
<p>I think of RL algorithms as a way to imitate the learning processes used by humans, and other animals, and often mimics known nuerological phenomoman observed in the biology fields.</p>
<section id="deep-reinforcment-learning-explained" class="level4">
<h4 class="anchored" data-anchor-id="deep-reinforcment-learning-explained">Deep reinforcment learning explained</h4>
<p>Use a deep neural network for your <code>policy</code> and you have deep RL.</p>
<hr>
</section>
</section>
</section>
<section id="how-to-get-value" class="level2">
<h2 class="anchored" data-anchor-id="how-to-get-value">How to get value</h2>
<p>The five goals listed above map into five actionable categories:</p>
<ol type="1">
<li>take action on the <code>fatherhood</code> and <code>developer</code> actions immediately in your day-to-day living (each article will include examples on how to do this)</li>
<li>relate my summaries of technologies and the projects I‚Äôm building to your own working experience</li>
<li>challenge me on my fatherhood beliefs if you disagree, and if you find your own beliefs challenged‚Äìthink deeply about that</li>
<li>experiment with health changes to a rational degree (we have different bodies, be aware)</li>
<li>learn from, then emulate, the results of my content creation and marketing strategies</li>
</ol>
<p><strong>For dads:</strong></p>
<p>Success and life fulfillment come as a package. You need to have it all!</p>
<p>If for example, you don‚Äôt agree with the importance of having great physical and spirtual health, then how will your children be healthy?</p>
<p>You know best for your family.</p>
<p>Don‚Äôt trust homeschooling only because I do it and don‚Äôt put your kid into a public school because you don‚Äôt know better. Have an intent behind your action.</p>
<p><strong>For everybody else:</strong></p>
<p>The fatherhood components to my writing will not always be present and will be designated with a <code>fatherhood</code> tag for those who do not benefit from dad-related content.</p>
<p>Maybe you have a father present in your life, such as your dad or a relative/friend with children, in which case you can help them out.</p>
<p>This blog got started because my wife shares lots of relevant dad content with me and I wanted to share my thoughts and beliefs as they develop. Writing solidifes concepts!</p>
<p>If you only benefit from ML developer content‚Äìawesome! Enjoy the content, I will post a lot of it.</p>
</section>
<section id="why-sovereign-tech-dad" class="level2">
<h2 class="anchored" data-anchor-id="why-sovereign-tech-dad">Why ‚ÄúSovereign‚Äù Tech Dad</h2>
<p>We live in an era of constant attention-seeking products and government welfare states. To state the obvious logic, if you don‚Äôt have independence from these systems‚Äìyou depend on them.</p>
<p>Independence promotes you to take meaningful risks. Get enough of it and your behavior will change to emphasize self-ownership and rational thought grounded in deterministic goals (goodbye fatalists üëã) instead of consumption as a solution.</p>
<p>Ideally, you should recognize to some degree the importance of globally integrated economies. What happens in China or Europe impacts North America and vice-versa.</p>
<p>Use emerging digital solutions to put yourself into a location-independent position. You should live and work because you love it there and you get good tax benefits‚Äìnot because you can‚Äôt feed your family if you move 40 miles.</p>
<p>I want you to benefit from changes on the global scale. Nationalism only works when your side wins, so don‚Äôt pit yourself against the rally when you could instead gain from it.</p>
</section>
<section id="final-thoughts-and-a-call-to-action" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts-and-a-call-to-action">Final thoughts and a call to action</h2>
<p>Pick the parts of the blog relevant to you.</p>
<p>Build valuable products, learn everyday, treat your family well, be healthy, and make money in a way that leads to independence.</p>
<p>I want to hear about your success! DM me or tweet. I would love nothing more then to show case the work and achievements of others!</p>


</section>
</section>

</main> <!-- /main -->
<div>
    <hr>
    
    <h3 class="anchored"> Stay in touch </h3>

    <ul>
        <li>email me at <a href="mailto:ogjaylowe@gmail.com">ogjaylowe@gmail.com</a></li>
        <li>follow and tweet at me on <a href="https://twitter.com/ogjaylowe">Twitter (@ogjaylowe)</a></li>
        <li>follow and DM at me on <a href="https://www.linkedin.com/in/jay-lowe-a21075163/">LinkedIn</a></li>
    </ul>

    <hr>
</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>